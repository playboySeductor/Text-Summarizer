{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6LLAapLThZm"
      },
      "outputs": [],
      "source": [
        "# article_text = \"\"\"Automatic text summarization has been an active field of research for many years . Several approaches have been proposed ranging from simple position and word frequency methods to learning and graph based algorithms . The advent of human generated knowledge bases like Wikipedia offer a further possibility in text summarization they can be used to understand the input text in terms of salient concepts from the knowledge base . In this paper we study a novel approach that leverages Wikipedia in conjunction with graph based ranking . Our approach is to first construct a bipartite sentence concept graph and then rank the input sentences using iterative updates on this graph . We consider several models for the bipartite graph and derive convergence properties under each model . Then we take up personalized and query focused summarization where the sentence ranks additionally depend on user interests and queries respectively . Finally we present a Wikipedia based multi document summarization algorithm . An important feature of the proposed algorithms is that they enable real time incremental summarization users can first view an initial summary and then request additional content if interested . We evaluate the performance of our proposed summarizer using the ROUGE metric and the results show that leveraging Wikipedia can significantly improve summary quality . We also present results from a user study which suggests that using incremental summarization can help in better understanding news articles .\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "N1GoLHMfT-Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dab1e21-cddf-4aa6-933d-9f39b06f640e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/train_ts.csv')"
      ],
      "metadata": {
        "id": "VPIBWa4-Gtac"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import csv\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  article_text = row['Abstract'].lower()\n",
        "  # remove spaces, punctuations and numbers\n",
        "  clean_text = re.sub('[^a-zA-Z]', ' ', article_text)\n",
        "  clean_text = re.sub('\\s+', ' ', clean_text)\n",
        "  # split into sentence list\n",
        "  sentence_list = nltk.sent_tokenize(article_text)\n",
        "  # word frequencies\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "  word_frequencies = {}\n",
        "  for word in nltk.word_tokenize(clean_text):\n",
        "    if word not in stopwords:\n",
        "      if word not in word_frequencies:\n",
        "        word_frequencies[word] = 1\n",
        "      else:\n",
        "        word_frequencies[word] += 1\n",
        "\n",
        "  # maximum frequency\n",
        "  maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "  for word in word_frequencies:\n",
        "    word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
        "\n",
        "  # sentence score\n",
        "  sentence_scores = {}\n",
        "\n",
        "  for sentence in sentence_list:\n",
        "    for word in nltk.word_tokenize(sentence):\n",
        "      if word in word_frequencies and len(sentence.split(' ')) < 30:\n",
        "        if sentence not in sentence_scores:\n",
        "          sentence_scores[sentence] = word_frequencies[word]\n",
        "        else:\n",
        "          sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "  # get top 5 sentences\n",
        "  summary = heapq.nlargest(1 , sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "  output = \" \".join(summary)\n",
        "\n",
        "  with open('output.csv', 'a') as f:\n",
        "    f.write(str(index + 1) + ') ' + output + '\\n')\n",
        "  # print(str(index + 1) + ') ' + output + '\\n')\n"
      ],
      "metadata": {
        "id": "d6ZnocmOIVHr"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}